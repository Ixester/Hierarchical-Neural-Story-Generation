{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Coleta\n"
      ],
      "metadata": {
        "id": "grixXnvDwo4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fairseq\n"
      ],
      "metadata": {
        "id": "j5oOAZ0oc9W8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce4ff77-15f7-4b3e-9a8e-738228b4d091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.5)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.8.3)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pr√© processamento"
      ],
      "metadata": {
        "id": "iMoCBmU6w9oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-preprocess --source-lang wp_source --target-lang wp_target \\\n",
        "    --trainpref examples/stories/writingPrompts/train --validpref examples/stories/writingPrompts/valid --testpref examples/stories/writingPrompts/test \\\n",
        "    --destdir data-bin/writingPrompts --padding-factor 1 --thresholdtgt 10 --thresholdsrc 10\n"
      ],
      "metadata": {
        "id": "Uq6I4JDkdQ4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9dcf3c-1f85-44cd-f37a-82f0bfdf2118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-17 20:11:02.869583: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-17 20:11:02.869672: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-17 20:11:02.869730: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-17 20:11:02.883340: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-17 20:11:04.506687: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-17 20:11:09 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-11-17 20:11:09 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='wp_source', target_lang='wp_target', trainpref='examples/stories/writingPrompts/train', validpref='examples/stories/writingPrompts/valid', testpref='examples/stories/writingPrompts/test', align_suffix=None, destdir='data-bin/writingPrompts', thresholdtgt=10, thresholdsrc=10, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=1, workers=1, dict_only=False)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-preprocess\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/preprocess.py\", line 389, in cli_main\n",
            "    main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/preprocess.py\", line 340, in main\n",
            "    src_dict = _build_dictionary(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/preprocess.py\", line 87, in _build_dictionary\n",
            "    return task.build_dictionary(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 114, in build_dictionary\n",
            "    Dictionary.add_file_to_dictionary(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/data/dictionary.py\", line 354, in add_file_to_dictionary\n",
            "    offsets = find_offsets(local_file, num_workers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/file_chunker_utils.py\", line 25, in find_offsets\n",
            "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'examples/stories/writingPrompts/train.wp_source'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Treinamento"
      ],
      "metadata": {
        "id": "hpOmI3EzxmnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-train data-bin/writingPrompts -a fconv_self_att_wp --lr 0.25 --optimizer nag --clip-norm 0.1 --max-tokens 1500 --lr-scheduler reduce_lr_on_plateau --decoder-attention True --encoder-attention False --criterion label_smoothed_cross_entropy --weight-decay .0000001 --label-smoothing 0 --source-lang wp_source --target-lang wp_target --gated-attention True --self-attention True --project-input True --pretrained False\n"
      ],
      "metadata": {
        "id": "DYr2KKqBdZGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb1bbda-37fe-4ee0-bc39-10fe28b39703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-17 20:11:21.231403: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-17 20:11:21.231484: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-17 20:11:21.231517: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-17 20:11:21.240646: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-17 20:11:23.066793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-17 20:11:25 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-17 20:11:27 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-11-17 20:11:29 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1500, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1500, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='reduce_lr_on_plateau', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=1500, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=1500, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='fconv_self_att_wp', max_epoch=0, max_update=0, stop_time_hours=0, clip_norm=0.1, sentence_avg=False, update_freq=[1], lr=[0.25], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/writingPrompts', source_lang='wp_source', target_lang='wp_target', load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.0, report_accuracy=False, ignore_prefix_size=0, momentum=0.99, weight_decay=1e-07, lr_shrink=0.1, lr_threshold=0.0001, lr_patience=0, warmup_updates=0, warmup_init_lr=-1, pad=1, eos=2, unk=3, decoder_attention='True', encoder_attention='False', gated_attention='True', self_attention='True', project_input='True', pretrained='False', no_seed_provided=False, encoder_embed_dim=256, encoder_layers='[(128, 3)] * 2 + [(512,3)] * 1', decoder_embed_dim=256, decoder_layers='[(512, 4)] * 4 + [(768, 4)] * 2 + [(1024, 4)] * 1', decoder_out_embed_dim=256, multihead_self_attention_nheads=4, downsample='True', dropout=0.1, multihead_attention_nheads=1, encoder_attention_nheads=1, pretrained_checkpoint='', _name='fconv_self_att_wp'), 'task': {'_name': 'translation', 'data': 'data-bin/writingPrompts', 'source_lang': 'wp_source', 'target_lang': 'wp_target', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.0, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'nag', 'momentum': 0.99, 'weight_decay': 1e-07, 'lr': [0.25]}, 'lr_scheduler': {'_name': 'reduce_lr_on_plateau', 'lr_shrink': 0.1, 'lr_threshold': 0.0001, 'lr_patience': 0, 'warmup_updates': 0, 'warmup_init_lr': -1.0, 'lr': [0.25], 'maximize_best_checkpoint_metric': False}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 557, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 87, in main\n",
            "    task = tasks.setup_task(cfg.task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/__init__.py\", line 46, in setup_task\n",
            "    return task.setup_task(cfg, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/translation.py\", line 308, in setup_task\n",
            "    src_dict = cls.load_dictionary(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 94, in load_dictionary\n",
            "    return Dictionary.load(filename)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/data/dictionary.py\", line 226, in load\n",
            "    d.add_from_file(f)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/data/dictionary.py\", line 239, in add_from_file\n",
            "    raise fnfe\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/data/dictionary.py\", line 236, in add_from_file\n",
            "    with open(PathManager.get_local_path(f), \"r\", encoding=\"utf-8\") as fd:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data-bin/writingPrompts/dict.wp_source.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gera√ß√£o\n"
      ],
      "metadata": {
        "id": "DrNVy50DxxOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-generate data-bin/writingPrompts --path /path/to/trained/model/checkpoint_best.pt --batch-size 32 --beam 1 --sampling --sampling-topk 10 --temperature 0.8 --nbest 1 --model-overrides \"{'pretrained_checkpoint':'/path/to/pretrained/model/checkpoint'}\"\n"
      ],
      "metadata": {
        "id": "xK2YnWxDdeo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd writingPrompts\n",
        "!ls\n",
        "\n"
      ],
      "metadata": {
        "id": "169B2HU-Quid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484dd636-00be-4aa1-deba-d8788a725eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/writingPrompts/writingPrompts/writingPrompts\n",
            "README\t\ttest.wp_target\t train.wp_target  valid.wp_target\n",
            "test.wp_source\ttrain.wp_source  valid.wp_source\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p examples/stories\n",
        "%cd examples/stories/writingPrompts\n",
        "!curl https://dl.fbaipublicfiles.com/fairseq/data/writingPrompts.tar.gz | tar xvzf -"
      ],
      "metadata": {
        "id": "vupvWgkpzyg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"train\", \"test\", \"valid\"]\n",
        "for name in data:\n",
        "    with open(name + \".wp_target\") as f:\n",
        "        stories = f.readlines()\n",
        "    stories = [\" \".join(i.split()[0:1000]) for i in stories]\n",
        "    with open(name + \".wp_target\", \"w\") as o:\n",
        "        for line in stories:\n",
        "            o.write(line.strip() + \"\\n\")\n"
      ],
      "metadata": {
        "id": "xNMaeqPcSf5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exemplo"
      ],
      "metadata": {
        "id": "S8drTanwz3LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 3 train.wp_target\n"
      ],
      "metadata": {
        "id": "NAiZLYEfTXv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f76806-08c7-4bf1-b765-1f236c38870d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So many times have I walked on ruins , the remainings of places that I loved and got used to.. At first I was scared , each time I could feel my city , my current generation collapse , break into the black hole that thrives within it , I could feel humanity , the way I 'm able to feel my body.. After a few hundred years , the pattern became obvious , no longer the war and damage that would devastate me over and over again in the far past was effecting me so dominantly . <newline> It 's funny , but I felt as if after gaining what I desired so long , what I have lived for my entire life , only then , when I achieved immortality I started truly aging . <newline> <newline> 5 world wars have passed , and now they feel like a simple sickeness that would pass by every so often , I could no longer evaluate the individual human as a being of its own , the importance of mortals is merely the same as the importance of my skin cells ; They are a part of a mechanism so much more advanced , a mechanism that is so dear to my fallen heart a mechanism that I have seen fall and rise so many times , a mechanism that when lost all of which it had , had me loosing my will to live , for the first time in all of my thousands years of existence . <newline> <newline> Acceptance , something so important . a skill that has proved itself worthy dozens of times , an ability that looks so easy to achieve , a gift , that I was n't able to aquire in all my years , until now . When the ashes on the ground flew into the now empty air upon humanity 's fall , I felt as if all of it 's weight was crushing me . Ignorance took over and I searched years for a hope , a sign of the very same patterns that I used to watch reappear every hundred years , the very core of my will to exist that was now no more that I so strongly wish was . <newline> <newline> If you have ever wondered if silence can drive people crazy , it can.. <newline> I ca n't feel my legs , I have walked for days , just to hear the sound of gravel , crushed bones , crushed buildings and crushed civilizations under my steps to keep my sanity.. until I remembered , the day in my far past . The day of my rebirth , I took out of my pocket a small plastic box , with nine buttons and a small glass window . I could n't believe this was our past , I could n't believe how far we have been able to progress and yet , be destroyed by our own violence . <newline> I slowly dialed the number I was given , exactly 1729 years ago . <newline> <newline> I dropped a tear , a tear that was too slow to hit the ground as I got sucked into the darkness that emerged around me . <newline> <newline> A chill went through my spine as I saw my destiny rise above me , I could see the white teeth under the dark cloack ... <newline> <newline> `` You have finally arrived '' He projected into my mind , with the most chilling cold and unhuman voice . <newline> <newline> `` I 'm ready to obey '' I answered . I knew who was sitting infront of me , and it was time for me to obey him , after all these years of playing god , even I came to it . <newline> <newline> Funny is n't it ? Even by achieving immortality , death , is inescapable .\n",
            "-Week 18 aboard the Depth Reaver , Circa 2023- <newline> <newline> I walk about the dull gray halls , the artificial gravity making my steps feel almost as if they were on land . Almost . I glance out a window as I pass it by . There 's the sun , and there 's the moon right there . And , of course , there 's the Earth . I kinda miss it . Then again , space is pretty cool . It 's got some brilliant views , and the wifi is surprisingly good . Even countless miles away from the Earth , I can crush Silver noobs on CS GO . <newline> <newline> I pass by Dale Malkowitz , the head scientist on board . <newline> <newline> `` Evening , Dale , '' I say . <newline> <newline> `` What up , Danny ? '' he replies cordially . <newline> <newline> `` Nothin ' much . A little bored , I guess . '' <newline> <newline> He shakes his head in disbelief . `` I really , *really* do n't understand how you can be bored in space . '' <newline> <newline> `` Well hey , '' I say slightly defensively , `` Aside from the views , it 's kinda ... dull . And empty . And stuff . '' <newline> <newline> `` Whatever you say , Wittell , '' he says , not unkindly . Then he walks off . A few moments pass , and then I decide to look out the window right by me . As my eyes scan the inky blackness of space ( again ) , I notice something odd about the moon 's surface . It 's slightly ... cracked . <newline> <newline> `` Hey , Malkowitz ? '' I call out , `` You might wan na check this out ! '' <newline> <newline> He walks over to me casually , probably expecting nothing . `` What ? '' he asks , `` What do you see ? '' <newline> <newline> I point at the moon . His brow furrows . `` Huh ... I guess there 's something up with the surface . I 'll have to look into tha- '' <newline> <newline> Suddenly , the surface cracks a little more . We glance at each other , and then back at the moon , and then at each other again , and then back at the moon again . <newline> <newline> `` What 's going on ? '' I ask , alarmed . <newline> <newline> He 's silent for a minute or two , mouth hanging open . Then , he calls out : `` Janice ! Terry ! Johnny ! Get over here ! Something 's up with the moon . '' <newline> <newline> The other crewmates enter , unsure of what to expect . As their eyes lay upon the moon 's surface cracks , they widen . <newline> <newline> And , by coincidence , more cracks appear at that very moment . And then more . And more . And more . And more ... <newline> <newline> Little bits of the moon begin to float away , torn free of the rest of the surface . We all stare , speechless . And then ... it happens . It *happens* . <newline> <newline> The side of the moon facing us is ... torn away by a ... <newline> <newline> Human ... hand ? <newline> <newline> And we see ... <newline> <newline> A giant ... human face ? ! <newline> <newline> Surprisingly , I can hear my thoughts over my racing heart . *I ca n't help but feel as if I recognize that face ... from the ... * <newline> <newline> *Internet . * <newline> <newline> Suddenly , the great face 's lips move . <newline> <newline> Of course , none of us can actually *hear* it speak , because of the laws of space and whatnot . However , I can read its lips , and it appears to be saying : <newline> <newline> `` Are you sure about that ? ''\n",
            "I was feckin ' sloshed , mate . First time I ever was in the Big Lemon , and I 'd found me the best feckin ' pub I could imagine , I tell ya what . So I stumble out when it was closin ' time , musta been 'round 4 o'clock in the morning , and made my way through some alleys to find the quaint little AirBnB place I 'd rented for the week . <newline> <newline> A'course , that 's how many a horror story starts , ainnit ? But it was all fun and games at first . There was this bloke I saw comin ' towards me in the alley , dark as it was with only a few lights from some apartments overhead , where the folk were still awake . At least , I thought it was a bloke , but he looked more like there was n't something right with the whole alleyway . Like it was a painting someone had gone and ripped with a knife for some reason , fecked-up as it all looked , and the cut looked sort of weird and silvery , and wavy like a heat haze comin ' from a welder 's torch . <newline> <newline> Now this thing must 've been a good foot or five taller 'n me , but sloshed as I am , I just take off my hat and give it a good `` how-do-you-do '' , 'cause I 'm a nice fellow and I wan na pass , y'see ? Now this thing up and goes give me a `` mighty fine , how about y'self , sir ! '' I feckin ' quite pissed my britches ! More in surprise 'n anythin ' , really , but I keep my composure and just sorta try and pass 'm as he walks past me with his long , spindly legs as silvery as the rest of 'm , with his feet makin ' noises like he 's wearin ' cowboy boots with those thingies on 'm . But then that moment stupid old me finds I forgot my lighter at the pub , and so I turn and ask spindleboots there if he 's got some fire for me . <newline> <newline> Darn buggerer turns around on the spot faster 'n my eyes can see and says , he says `` sure ! I got your fire here . '' And the feckin ' thing stretches out one of his spindly legs and wraps it around me like a cobra and pulls me into itself . Devil 's pits , that felt like I was bein ' burnt layer of skin by layer . He sure got me with that . Good feckin ' joke , mate . <newline> <newline> Must 've been out like a light for ages , but when I woke up , I find myself here . So tell me , Doc : are time travellers really that sought after as subjects of study ? 'Cause I know you found me in your fancy cyberwebs database thing and I ca n't be three hundred years old as my passport would say if I had one , but I 'd really like to not die , y'know ? What do y'say , wan na go and grab a beer at the pub ?\n"
          ]
        }
      ]
    }
  ]
}